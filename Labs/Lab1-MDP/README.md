# Lab 1 Markov Decision Processes

Check out the pdf in this folder to get started. 

Outline of this lab:

#### 1. SIMPLE MDP
- Based on problem 1 in this [Stanford Course](http://web.stanford.edu/class/cs234/assignment1/assignment1.pdf)
- Additionally show: In policy iteration, the update step is guaranteed to improve the policy.

#### 2. CLIFF WORLD - ON VS OFF POLICY
- [Environment](http://ai.berkeley.edu/reinforcement.html#Q3)
- Learn effects of parameters such as discount, noise, reward
- On/off policy comparison (either discuss for this problem or implement)
- Analysis of importance sampling (variance vs. bias, section 17.1.3 of DL textbook)
- Based on exercise 9.10 in this [Stanford Course](https://statweb.stanford.edu/~owen/mc/Ch-var-is.pdf) 
  or 5.7/5.8 in this [Alberta Course](https://sites.ualberta.ca/~szepesva/CMPUT607/Questions5e.pdf) 

#### 3. PACMAN
- [Environment](http://ai.berkeley.edu/reinforcement.html#Q7)
- Start with: SmallGrid (trains in a few minutes)
- If time / over night: SmallClassic
- Play around with different values, plot rewards vs. updates
- How discounting, reward and stochasticity affect policy? (when it matters, what could be the problem)

# 

#### Resources
- Stanford, Emma Brunskill [[Exercises]](https://drive.google.com/open?id=1BXX_5sfmFDBaxljhMPzXC_i1vUzd8AoX)
- Alberta, Rich Sutton [[Exercises]](https://drive.google.com/drive/folders/0B3w765rOKuKANmxNbXdwaE1YU1k)
- CMU, Emma Brunskill [[Exercises]](http://www.cs.cmu.edu/~ebrun/15889e/homeworks.html)
- Berkeley, Sergey Levine
- Deep RL bootcamp, Berkeley [[Lectures]](https://sites.google.com/view/deep-rl-bootcamp/lectures)
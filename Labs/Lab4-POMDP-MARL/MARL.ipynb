{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import operator\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 1: Tabular Independent Q-learning\n",
    "In this exercise, you will see how multiple agents can learn behaviours simultaneously with reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block, we define a Predator-Prey environment. The predators are agents which will move in a toroidal grid, attempting to catch a prey. Capture is successful when a predator moves onto the prey, and another predator is adjacent to the prey to support it.\n",
    "\n",
    "If the capture fails, predators move onto each other, or capture is successful, the predators are moved to random positions.\n",
    "\n",
    "This environment is continuing by default, so there are no terminal states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PredPrey:\n",
    "    def __init__(self, n_agents, h, w):\n",
    "        self.n_agents = n_agents\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.n_cells = h*w\n",
    "        self.pred_pos = [(0,0) for _ in range(n_agents)]\n",
    "        self.prey_pos = np.zeros((2))\n",
    "        # seed control for reproducibility\n",
    "        self.rng = random.Random(0)\n",
    "\n",
    "        # Reward function parameters\n",
    "        self.living_reward = 0.0\n",
    "        self.pred_stack_reward = -10\n",
    "        self.capture_reward = 37.5\n",
    "        self.fail_capture_reward = -25\n",
    "\n",
    "        # Prey parameters\n",
    "        self.possible_prey_actions = ['north', 'west', 'south', 'east']\n",
    "        self.prey_move_prob = 0.8\n",
    "        \n",
    "        # Every agent can take one of these actions at each timestep.\n",
    "        self.possible_agent_actions = ('north','west','south','east','stay')\n",
    "        \n",
    "        \"\"\"\n",
    "        A joint action is a tuple with each agent's actions.\n",
    "        This property should be the list of all possible joint actions:\n",
    "        \"\"\"\n",
    "        self.possible_joint_actions = list(itertools.product(*[self.possible_agent_actions for _ in range(self.n_agents)]))\n",
    "        \n",
    "        # Define the directions\n",
    "        self.action_deltas = {'north': (-1, 0), 'west': (0, -1), 'south': (1, 0), 'east': (0, 1), 'stay': (0,0)}\n",
    "        self.reset()\n",
    "    \n",
    "    def get_reward(self):\n",
    "        if sum([(p == self.prey_pos) for p in self.pred_pos]) == 1:\n",
    "            # capture attempted, exactly one pred moved on prey\n",
    "            if any([np.abs(np.array(p) - np.array(self.prey_pos)).sum() == 1 for p in self.pred_pos]):\n",
    "                # at least one supporting predator\n",
    "                self.reset()\n",
    "                return self.capture_reward\n",
    "            else:\n",
    "                self.reset(predator_only=True)\n",
    "                return self.fail_capture_reward\n",
    "        elif len(self.pred_pos) != len(set(self.pred_pos)):\n",
    "            # stacked up predators\n",
    "            self.reset(predator_only=True)\n",
    "            return self.pred_stack_reward\n",
    "        else:\n",
    "            return self.living_reward\n",
    "    \n",
    "    def reset(self, predator_only=False):\n",
    "        positions = self.rng.sample(range(self.n_cells), self.n_agents + 1)\n",
    "        if not predator_only:\n",
    "            self.prey_pos = np.unravel_index(positions[0], (self.h, self.w))\n",
    "        self.pred_pos = [np.unravel_index(p, (self.h, self.w)) for p in positions[1:]]\n",
    "        return self.get_state()\n",
    "    \n",
    "    def _move(self, pos, delta):\n",
    "        return ((pos[0] + delta[0]) % self.h, (pos[1] + delta[1]) % self.w)\n",
    "    \n",
    "    def step(self, joint_action):\n",
    "        for a in range(self.n_agents):\n",
    "            self.pred_pos[a] = self._move(self.pred_pos[a], self.action_deltas[joint_action[a]])\n",
    "        reward = self.get_reward()\n",
    "        if self.rng.random() < self.prey_move_prob:\n",
    "            # Move prey to a random free adjacent space\n",
    "            random.shuffle(self.possible_prey_actions)\n",
    "            for prey_action in self.possible_prey_actions:\n",
    "                possible_prey_pos = self._move(self.prey_pos, self.action_deltas[prey_action])\n",
    "                if possible_prey_pos not in self.pred_pos:\n",
    "                    self.prey_pos = possible_prey_pos\n",
    "\n",
    "        return (self.get_state(), reward, False, {})\n",
    "    \n",
    "    def get_state(self):\n",
    "        prey_x, prey_y = self.prey_pos\n",
    "        return tuple([ ( (pred_y - prey_y)%self.h, (pred_x - prey_x)%self.w ) for (pred_x, pred_y) in self.pred_pos])\n",
    "    \n",
    "    def display(self):\n",
    "        grid = np.zeros((self.h, self.w))\n",
    "        for a, pos in enumerate(self.pred_pos):\n",
    "            grid[pos] = a + 1\n",
    "        grid[self.prey_pos] = -1\n",
    "        print(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get familiar with the environment, play around with the code blocks below or make your own. Choose your own actions to control the predators. Observe how the predators and prey move, and how the agents must coordinate to capture the prey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a predator-prey environment\n",
    "env = PredPrey(2, 4, 4)\n",
    "env.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s, r, terminal, _ = env.step((\"north\", \"south\"))\n",
    "print(\"Reward \", r)\n",
    "env.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is the size of the joint action space? Complete the initialisation of env.possible_joint_actions to contain a list of all joint actions. Each joint action should be a tuple of agent actions.\n",
    "\n",
    "(Hint: this can be a one-liner with itertools.product)\n",
    "\n",
    "**Question**: How many possible states are there, given the height and width of the grid (h,w) and the number of agents N? Assume a single prey always. Predators and prey are not allowed to overlap.\n",
    "\n",
    "**Question**: Look at env.get_state(). How is the state represented? Does this correspond to the number of states you were expecting? Did the postion of the prey matter? Could we reduce the state-action space further -- are any state-action pairs equivalent with each other in terms of their value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code-block contains a plotting function that keeps track of the experimental results. Re-executing this block will clear all previous experiments from the plots; otherwise it can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_data = {}\n",
    "grid_sizes = []\n",
    "plt.clf()\n",
    "plt.close()\n",
    "\n",
    "def plot_all_results(plot_std=False):\n",
    "    global grid_sizes\n",
    "    colors = ['orange', 'red', 'magenta', 'blue', 'green', 'black', 'c']\n",
    "    graphs = len(grid_sizes) + 1 # Episode rewards for each grid size and table sizes\n",
    "    fig, ax = plt.subplots(1, graphs, figsize=[7*graphs, 5])\n",
    "    grid_sizes = sorted(grid_sizes)\n",
    "    # Plot the performance\n",
    "    for exp_id, exp in enumerate(plot_data.keys()):\n",
    "        data = plot_data[exp]\n",
    "        plot_ts = data[\"t\"]\n",
    "        plot_rewards = data[\"captures\"]\n",
    "        plot_table_sizes = data[\"table_size\"]\n",
    "        grid_index = 0\n",
    "        for gidx, gs in enumerate(grid_sizes):\n",
    "            if gs in exp:\n",
    "                grid_index = gidx\n",
    "                break\n",
    "        gca = ax[gidx]\n",
    "        for ts, rewards in zip(plot_ts, plot_rewards):\n",
    "            if rewards is not None:\n",
    "                gca.plot(ts, rewards, label=exp, color=colors[exp_id%len(colors)])\n",
    "        gca.set_xlabel(\"Env steps\")\n",
    "        gca.set_ylabel(\"Captures / 1000 steps\")\n",
    "        gca.title.set_text(f\"{grid_sizes[grid_index]}\")\n",
    "        gca = ax[graphs-1]\n",
    "        for ts, table_sizes in zip(plot_ts, plot_table_sizes):\n",
    "            gca.plot(ts, table_sizes, label=exp, color=colors[exp_id%len(colors)])\n",
    "        gca.set_xlabel(\"Env steps\")\n",
    "        gca.set_ylabel(\"Size of Q Table\")\n",
    "        gca.set_yscale(\"log\")\n",
    "        # Removing duplicates from legend. From https://stackoverflow.com/questions/13588920/stop-matplotlib-repeating-labels-in-legend\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        by_label = OrderedDict(zip(labels, handles))\n",
    "        gca.legend(by_label.values(), by_label.keys())\n",
    "#         gca.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1a: Another Tabular Q-Learner\n",
    "Familiarise yourself with the implementation of a tabular Q-Learner below.\n",
    "\n",
    "**Question**: Look at the calculation of the greedy choice line. Is there a problem with this implementation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TabularQLearner:\n",
    "    \"\"\" Tabular Q-learning agent. \"\"\"\n",
    "    gamma = 0.9\n",
    "    learn_rate = 0.1\n",
    "\n",
    "    def __init__(self, actions_list):\n",
    "        self.name = \"SingleLearner\"\n",
    "        self.actions_list = actions_list\n",
    "        self.q_table = defaultdict(lambda: {a: 0.0 for a in actions_list})\n",
    "        \n",
    "    def q_values(self, state):\n",
    "        return self.q_table[state]\n",
    "\n",
    "    def sample(self, state, eps):\n",
    "        \"\"\" Sample an action epsilon-greedy. \"\"\"\n",
    "        if np.random.uniform(0, 1) < eps: # Random choice\n",
    "            action = random.choice(list(self.q_table[state].keys()))\n",
    "        else: # Greedy choice\n",
    "            action = max(self.q_table[state].items(), key=operator.itemgetter(1))[0]\n",
    "        return action\n",
    "\n",
    "    def value(self, state):\n",
    "        \"\"\" Returns the value of the state. \"\"\"\n",
    "        return max(self.q_table[state].values())\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\" The agent 'learns' from the given transition. \"\"\"\n",
    "        not_done = 0 if done else 1\n",
    "        td_error = reward + not_done * self.gamma * self.value(next_state) - self.q_table[state][action]\n",
    "        self.q_table[state][action] = self.q_table[state][action] + self.learn_rate * td_error\n",
    "    \n",
    "    def stats(self):\n",
    "        return {\"table_size\": len(self.q_table.keys())*len(self.actions_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1b: the experiment \n",
    "Familiarize yourself with the basic loop of the RL experiment.\n",
    "\n",
    "**Question:** There is an option to disable training during a phase of zero-epsilon evaluation. Do we technically need to disable training durning this phase? Why / why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_steps(env, agent, n_steps, eps, training=True):\n",
    "    state = env.get_state()\n",
    "    all_rewards = []\n",
    "    all_captures = []\n",
    "    # Run the environment continuously -- there are no terminal states in the default pred-prey\n",
    "    for step in range(n_steps + 1):\n",
    "        current_state = state\n",
    "        action = agent.sample(current_state, eps)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        all_rewards.append(reward)\n",
    "        all_captures.append(reward > 0)\n",
    "        if training:\n",
    "            agent.update(current_state, action, reward, state, done)\n",
    "    return all_rewards, all_captures\n",
    "\n",
    "def run_experiment(env, agent, num_epochs=20, steps_epoch=int(5e4), plot_rewards=True):\n",
    "    steps_eval = int(2e4)\n",
    "    eps = 0.1 # epsilon for epsilon-greedy exploration\n",
    "    # Set seeds (for reproduceability)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    print(f'----- Start Learning with {agent.name}, {env.n_agents} agents -----')\n",
    "    ts, all_eval_captures, all_train_captures, all_table_sizes = [], [], [], []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        t_start = time.time()\n",
    "        env.reset()\n",
    "        train_rewards, train_captures = run_steps(env, agent, steps_epoch, eps, training=True)\n",
    "        env.reset()\n",
    "        eval_rewards, eval_captures = run_steps(env, agent, steps_eval, 0.0, training=False)\n",
    "        t_finish = time.time()\n",
    "        stats = agent.stats()\n",
    "        print(f'Epoch {epoch}: time = {t_finish-t_start:.2f}')\n",
    "        print(f'Train: reward = {np.mean(train_rewards):.2f} -- Captures/1000 = {np.sum(train_captures)/(steps_epoch/1000):.2f}')\n",
    "        print(f'Eval: reward = {np.mean(eval_rewards):.2f} -- Captures/1000 = {np.sum(eval_captures)/(steps_eval/1000):.2f}')\n",
    "        print(stats)\n",
    "        ts.append(epoch*steps_epoch)\n",
    "        all_train_captures.append(np.sum(train_captures)/(steps_epoch/1000))\n",
    "        all_eval_captures.append(np.sum(eval_captures)/(steps_eval/1000))\n",
    "        all_table_sizes.append(stats[\"table_size\"])\n",
    "#   Plotting stuff\n",
    "    exp_name = agent.name + f'-{env.n_agents}-{env.h}x{env.w}'\n",
    "    if not plot_rewards:\n",
    "        all_train_captures = None\n",
    "    if exp_name not in plot_data:\n",
    "        plot_data[exp_name] = {\n",
    "            \"t\": [ts], \"captures\": [all_train_captures], \"table_size\": [all_table_sizes]\n",
    "        }\n",
    "    else:\n",
    "        plot_data[exp_name][\"t\"].append(ts)\n",
    "        plot_data[exp_name][\"captures\"].append(all_eval_captures)\n",
    "        plot_data[exp_name][\"table_size\"].append(all_table_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1c: test the tabular agent \n",
    "Run the experiment with a TabularQLearner. This is a single learner that controls all agents jointly as a puppeteer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PredPrey(2, 10, 10)\n",
    "run_experiment(env,\n",
    "               TabularQLearner(env.possible_joint_actions),\n",
    "               num_epochs=20, plot_rewards=True)\n",
    "plot_all_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = PredPrey(2, 4, 4)\n",
    "run_experiment(env,\n",
    "               TabularQLearner(env.possible_joint_actions),\n",
    "               num_epochs=5, plot_rewards=True)\n",
    "plot_all_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1d: Scaling of joint action learning\n",
    "Let's revisit the size of the joint action space. \n",
    "\n",
    "Run the experiment with more agents, and observe how the size of the Q-Table increases. Notice the time per epoch increasing as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n_agents in range(3, 5):\n",
    "    env = PredPrey(n_agents, 4, 4)\n",
    "    run_experiment(env,\n",
    "                   TabularQLearner(env.possible_joint_actions),\n",
    "                   num_epochs=5, plot_rewards=False)\n",
    "plot_all_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 2: Independent Q-Learning\n",
    "In this exercise you will implement independent Q-Learning!\n",
    "This will address the scaling of a joint action learner, and permit decentralised learning or execution of policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2a: Build an interface for independent learners\n",
    "Build a IndependentQLearner that manages several TabularQLearners!\n",
    "This should be callable by the experiment, so it needs to return joint actions.\n",
    "But instead of being backed by a single TabularQLearner, make it use one learner for each agent. Each agent can still see the full state. Also, answer the question there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IndependentQLearner:\n",
    "    \"\"\" Tabular Q-learning agent. \"\"\"\n",
    "    gamma = 0.9\n",
    "    learn_rate = 0.2\n",
    "\n",
    "    def __init__(self, env, agent_learner=TabularQLearner):\n",
    "        \"\"\" Initialise one agent_learner for each agent \"\"\"\n",
    "        self.name = f\"IQL: {agent_learner.__name__}\"\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def sample(self, state, eps):\n",
    "        \"\"\" Sample for each agent independently \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def value(self, state):\n",
    "        \"\"\" Why can't we return a value here? \"\"\"\n",
    "        assert False\n",
    "\n",
    "    def update(self, state, joint_action, reward, next_state, done):\n",
    "        \"\"\" Update each agent's learner with the shared experience. \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "    \n",
    "    def stats(self):\n",
    "        \"\"\" Return the sum (by key) of stats dicts for each learner. \"\"\"\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2b: Test the independent Q-Learner\n",
    "Run the independent Q-learner below. It should get some decent results, comparable to that of the joint-action learner. But the training should be more unstable.\n",
    "\n",
    "**Question**: Why is IQL potentially unstable? Why might it not converge? How does exploration interact with learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PredPrey(2, 10, 10)\n",
    "run_experiment(env, IndependentQLearner(env), num_epochs=20)\n",
    "plot_all_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = PredPrey(2, 4, 4)\n",
    "run_experiment(env, IndependentQLearner(env), num_epochs=5)\n",
    "plot_all_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c: Investigate the scaling of IQL\n",
    "\n",
    "Check how IQL compares to Joint-action Q-Learning in scaling up with more agents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n_agents in range(3, 5):\n",
    "    env = PredPrey(n_agents, 4, 4)\n",
    "    run_experiment(env,\n",
    "                   IndependentQLearner(env),\n",
    "                   num_epochs=5, plot_rewards=False)\n",
    "plot_all_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 3: Hysteretic Q-Learning\n",
    "\n",
    "## Task 3a: Re-invent hysteretic Q-Learning (with some hints)\n",
    "Think about how exploration interacts with the values seen by the agents.\n",
    "When one agent takes a greedy action but another explores, will the independent learner over- or under- estimate the optimal value of the state? Why? \n",
    "\n",
    "Given that, should we rather trust more high or low bootstrap estimates of the optimal value? Why? \n",
    "\n",
    "Imagine using a different learning rate for positive and negative TD-errors. Would this give us a way to implement the above idea?\n",
    "\n",
    "In this case, should the learning rate be higher or lower for postive or negative TD-errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3a: Implement Hysteretic Q-Learning\n",
    "\n",
    "Make a hysteretic Q-learner that subclasses the TabularQLearner.\n",
    "Use the default learn_rate as the larger learning rate, and beta=0.01 as the smaller learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HystereticQLearner(TabularQLearner):\n",
    "    beta = 0.01\n",
    "    \n",
    "    def __init__(self, action_list):\n",
    "        super(HystereticQLearner, self).__init__(action_list)\n",
    "        self.name = \"Hysteretic\"\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\" This update uses a different learning rate for positive and negative TD-errors \"\"\"\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c: Test Hysteretic Q-Learning\n",
    "\n",
    "Test the hysteretic Q-learner. If all goes well, it should outperform the other variants.\n",
    "Think about why in retrospect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PredPrey(2, 10, 10)\n",
    "run_experiment(env,\n",
    "               IndependentQLearner(env, agent_learner=HystereticQLearner),\n",
    "               num_epochs=20)\n",
    "plot_all_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = PredPrey(2, 4, 4)\n",
    "run_experiment(env,\n",
    "               IndependentQLearner(env, agent_learner=HystereticQLearner),\n",
    "               num_epochs=5)\n",
    "plot_all_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (BONUS) Symmetry and Greed\n",
    "\n",
    "There are a number of symmetries in this problem. How can we use them to learn faster or with fewer parameters?\n",
    "\n",
    "(Parameter sharing between agents, spatial symmetries should reduce size of Q-table a lot)\n",
    "\n",
    "Why is the greedy policy so bad in evaluation compared to the epsilon-greedy one? Is there a bug?\n",
    "\n",
    "This practical has been based on the predator-prey environment and algorithms from this paper:\n",
    "https://hal.archives-ouvertes.fr/hal-00187279/document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

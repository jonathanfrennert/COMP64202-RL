{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WG8IxawUds-v"
   },
   "source": [
    "# Getting ready #\n",
    "\n",
    "The goal of this exercise is to understand *Value Iteration* in POMDPs.\n",
    "As there's quite a lot going on, we'll slowly walk through every logical step involved, hopefully developing some intuitive understanding of why POMDPs are difficult and how they can be solved.\n",
    "\n",
    "You should already understand the idea of *Value Iteration* in the fully observable case, i.e. in MDPs. Furthermore, we'll need *Bayes Rule* \n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A) P(A)}{P(B)},$$\n",
    "\n",
    "so make sure you know what's going on there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UJ0AUPl7iSnk"
   },
   "source": [
    "# Chapter 0: The illustrative example #\n",
    "\n",
    "In this exercise, we will be using the following example POMDP, taken from [here](http://ais.informatik.uni-freiburg.de/teaching/ws11/robotics2/pdfs/rob2-XX-POMDPs.pdf)\n",
    "\n",
    "![Graphical Model of example POMDP](https://github.com/maximilianigl/pomdp-practical/blob/master/images/Graph_model.png?raw=true)\n",
    "\n",
    "So, for rewards $r$, states $x_i$, actions $u_i$ and observations $z_i$ we have for the rewards in state $x_1$: \n",
    "$r(x_1, u_1) = -100$, $r(x_1, u_2) = 100$, $r(x_1, u_3) = -1$. \n",
    "\n",
    "The transition dynamics into state $x_1$ are given by: $p(x_1'|x_1, u_3) = 0.2$ and $p(x_1'|x_2, u_3)= 0.8$, i.e. there's only a chance to switch the state if we pick action $u_3$. Importantly, $u_3$ is the *sensing* action, i.e. we only get observations when we pick $u_3$. \n",
    "\n",
    "The observation probabilities in $x_1$ are given by $p(z_1|x_1) = 0.7$, $p(z_2|x_1) = 0.3$.\n",
    "Lastly, once we pick $u_1$ or $u_2$, we either get a big positive or negative reward *and the episode ends*.\n",
    "\n",
    "Below I already implemented the corresponding functions to represent that POMDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oCPz-DZklgxT"
   },
   "outputs": [],
   "source": [
    "rewards = [\n",
    "    [-100, 100, -1],\n",
    "    [100,  -50, -1]\n",
    "]\n",
    "def reward(state, action):\n",
    "  \"\"\"\n",
    "  Returns the reward for an `action` in a certain `state`. \n",
    "  \n",
    "  Args: \n",
    "    state (int): The state for which to return the reward\n",
    "    action (int): The action taken in state.\n",
    "\n",
    "  Returns:\n",
    "    int: The reward\n",
    "  \"\"\"\n",
    "  \n",
    "  return rewards[state-1][action-1]\n",
    "\n",
    "transitions = [\n",
    "    [0.2, 0.8],\n",
    "    [0.8, 0.2]\n",
    "]\n",
    "\n",
    "def transition_probs(state, action, new_state):\n",
    "  \"\"\"\n",
    "  Returns the probability to transition to `new_state` when taking action `action` in state `state`.\n",
    "  \n",
    "  Args:\n",
    "    state (int): The current state\n",
    "    action (int): Action taken in `state`\n",
    "    new_state (int): The new state in which to transition to\n",
    "    \n",
    "  Returns:\n",
    "    float: Probability to transition into new_state\n",
    "  \"\"\"\n",
    "  if action != 3:\n",
    "    return state == new_state\n",
    "  else:\n",
    "    return transitions[state-1][new_state-1]\n",
    "  \n",
    "observations = [\n",
    "    [0.7, 0.3],\n",
    "    [0.3, 0.7]\n",
    "]\n",
    "def observation_probs(new_state, action, obs):\n",
    "  \"\"\"\n",
    "  Returns the probability of observing `obs` in state `new_state` when taking `action`.\n",
    "  So when we take an action, we first transition into `new_state` and then generate an observation.\n",
    "  \n",
    "  (When not taking u_3 as action, set p(z_1|x_i) = 1 for both states, \n",
    "  i.e. we don't learn anything from the observation.)\n",
    "  \n",
    "  Args:\n",
    "    state (int): The current state\n",
    "    action (int): Action taken in `state`\n",
    "    obs (int): The observation whose probability we evaluate\n",
    "    \n",
    "  Returns:\n",
    "    float: Probability of observation\n",
    "  \"\"\"\n",
    "  if action != 3:\n",
    "    # Set p(z_1|x_i)=1 in both states if we're not using u_3\n",
    "    return obs == 1\n",
    "  else:\n",
    "    return observations[state-1][obs-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-o4FCA0Lgn-_"
   },
   "source": [
    "# Chapter 1: Planning in the belief MDP #\n",
    "\n",
    "As you should have learned in the lecture this morning, we can see the POMDP as a *Believe MDP*, e.g. as explained [here](https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process#Belief_MDP):\n",
    "\n",
    "*A Markovian belief state allows a POMDP to be formulated as a Markov decision process where every belief is a state. The resulting belief MDP will thus be defined on a continuous state space (even if the \"originating\" POMDP has a finite number of states: there are infinite belief states (in $B$) because there are an infinite number of mixtures of the originating states (of $S$), since there are infinite beliefs for any given POMDP.*\n",
    "\n",
    "Before reasoning about how to solve the POMDP by using Value Iteration to find the best action for *every* possible belief state, let's first try to find the best action to take for *one* starting belief state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eFj_N7cHk-M3"
   },
   "source": [
    "## 1.1 The 1-step horizon ##\n",
    "\n",
    "First, let's assume we can only take *one* action and then the episode ends. \n",
    "\n",
    "### The expected reward ###\n",
    "\n",
    "Given a belief $b=[p_1, 1-p_1]$ of being in state 1 or 2 respectively, what is the expected reward for taking action $u_1$ or $u_2$?\n",
    "\n",
    "**Question:**  Fill out the function below.\n",
    "\n",
    "**Note:** For the implementation, because we only have two states, we can represent the belief by $p_1$, i.e. the probability to be in state 1, with the remaining probability to be in state 2 being of course $1-p_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5A6BPqrudmAd"
   },
   "outputs": [],
   "source": [
    "def reward_b(p1, action):\n",
    "  \"\"\"\n",
    "  Compute the expected reward for taking a certain `action`.\n",
    "  \n",
    "  Args:\n",
    "    p1 (float): Belief to be in state 1\n",
    "    action (int): Action taken\n",
    "    \n",
    "  Returns:\n",
    "    float: Expected reward.\n",
    "  \n",
    "  \"\"\"\n",
    "  \n",
    "  \n",
    "  return #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IRW_-aajB44U"
   },
   "source": [
    "Let's visualize this for the different actions (just run the code below). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VdUi4KxzB4oV"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# With numpy, operations like +/-/*// are performed element-wise, \n",
    "# so unless you did something weird above, we can evaluate the values of all \n",
    "# beliefes in parallel.\n",
    "p1s = np.linspace(0,1)\n",
    "\n",
    "plt.plot(p1s, reward_b(p1s, action=1), color='red')\n",
    "plt.plot(p1s, reward_b(p1s, action=2), color='blue')\n",
    "plt.plot(p1s, reward_b(p1s, action=3), color='green')\n",
    "\n",
    "plt.xlabel(\"Belief to be in state 1\")\n",
    "plt.ylabel(\"Expected Reward\")\n",
    "\n",
    "# We'll need this one later\n",
    "# plt.plot(p1s, one_step_value(p1s), color='black', linestyle='--')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hvuAPzZlLN5J"
   },
   "source": [
    "**Question:** Why does this make sense? What's the best action to take in each belief state (assuming we can only take this one action and the episode ends afterwards)? Is the resulting function of 'best possible expected reward we can achieve in each belief state' convex ;)? If yes - why does this make intuitive sense? Is it piecewise linear? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mt7tXJHOLkNd"
   },
   "source": [
    "Because we'll need this in the 2-step case below, let's write a function which returns us the best achievable 1-step return for each possible belief\n",
    "\n",
    "**Question:**  Fill out the function below.\n",
    "\n",
    "(**Hint:** The numpy function you're looking for is `np.maximum` and not `np.max`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "r5nb_gaDECRd"
   },
   "outputs": [],
   "source": [
    "def one_step_value(p1):\n",
    "  \"\"\"\n",
    "  Compute the 1-step value for a given belief.\n",
    "  \n",
    "  Args:\n",
    "    p1 (float): Belief to be in state 1\n",
    "    \n",
    "  Returns:\n",
    "    float: Expected reward.\n",
    "  \n",
    "  \"\"\"\n",
    "  \n",
    "  return #TODO\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jPpAjPOZHYoX"
   },
   "source": [
    "Go back up to the plot, uncomment the remaining line and plot this function as well, just to check it does what it's supposed to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i1VIwY1pEC6E"
   },
   "source": [
    "## 1.2 Planning for with a 2 step horizon ## \n",
    "\n",
    "If we take $u_1$ or $u_2$, the episode ends immediately. However, when we take $u_3$ we actually get an observation (i.e. more information) and can take another action. So the value in the above plot for taking $u_3$ is not true if we look at an horizon of length $>1$. So let's look at next closest thing and fix the horizon length to 2. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "quq4ufjQrq1-"
   },
   "source": [
    "### Updating the belief ###\n",
    "\n",
    "The first thing we need to do is figure out how to update our belief when we take an action and get an observation back from the environment.\n",
    "\n",
    "**Question:** Please fill out the function below to compute the updated belief given the past belief, the action taken and the observation recieved from the environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sEeuff0BsASV"
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def prior_next_state(p1, action, next_state):\n",
    "  \"\"\"\n",
    "  Compute the probability p(next_state|action, p1), i.e. the prior probability\n",
    "  (before taking any observation into account)\n",
    "  to be in state `next_state` when taking `action`, given our current belief \n",
    "  as represented by p1\n",
    "  \n",
    "  Args:\n",
    "    p1 (float): Belief to be in state 1\n",
    "    action (int): Action taken\n",
    "    next_staet (int): Next state\n",
    "    \n",
    "  Returns:\n",
    "    float: Prior probability to be in state `next_state`\n",
    "  \n",
    "  \"\"\"\n",
    "  return # TODO\n",
    "\n",
    "\n",
    "\n",
    "def belief_update(p1, action, obs):\n",
    "  \"\"\"\n",
    "  Compute the new probability `p1` to be in state 1.\n",
    "  \n",
    "  Args:\n",
    "    p1 (float): Belief to be in state 1\n",
    "    action (int): Action taken\n",
    "    obs (int): Observation recieved from the environment\n",
    "    \n",
    "  Returns:\n",
    "    float: Updated belief to be in state 1\n",
    "  \n",
    "  \"\"\"\n",
    "  \n",
    " # TODO\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LpRw3k81PHBF"
   },
   "source": [
    "Ok, let's just visualize that again. Just execute the field below and think about whether the result makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "O2FTMb3AOMt7"
   },
   "outputs": [],
   "source": [
    "p1s = np.linspace(0,1)\n",
    "\n",
    "# Let's plot the belief to be in state 1 for both possible observations that we\n",
    "# Could get when taking action 3\n",
    "\n",
    "plt.plot(p1s, belief_update(p1s, action=3, obs=1), color='red')\n",
    "plt.plot(p1s, belief_update(p1s, action=3, obs=2), color='blue')\n",
    "plt.xlabel(\"Belief to be in state 1 originally\")\n",
    "plt.ylabel(\"Belief to be in state one after update\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KDmVbswiIE1c"
   },
   "source": [
    "### 2 step planning for actions $u_3$###\n",
    "\n",
    "**Question:** Ok, now, let's write a function that computes the *expected* two step return for taking action $u_3$. To help, I've provided the `simulate` function, that returns the probability for observation 1 given a certain belief $p_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "SuNu_DskGcAc"
   },
   "outputs": [],
   "source": [
    "def simulate(p1):\n",
    "  \"\"\"\n",
    "  Returns the probability for observation 1 when our current belief is given\n",
    "  by p1 and we exectue the sensing action u3\n",
    "  \n",
    "  Args:\n",
    "    p1 (float): Belief to be in state 1\n",
    "    \n",
    "  Returns:\n",
    "    float: probability to observe o1 when taking action 3\n",
    "  \"\"\"\n",
    "  p1_prime = (p1 * transition_probs(state=1, action=3, new_state=1) +\n",
    "             (1-p1) * transition_probs(state=2, action=3, new_state=1))\n",
    "  \n",
    "  p_o1 = (p1_prime * observation_probs(state=1, action=3, obs=1) + \n",
    "          (1-p1_prime) * observation_probs(state=2, action=3, obs=1))\n",
    "          \n",
    "  return p_o1\n",
    "  \n",
    "  \n",
    "\n",
    "def value_taking_u3_first(p1):\n",
    "  \"\"\"\n",
    "  Compute the 2-step value for a given belief when we take action u_3\n",
    "  \n",
    "  Args:\n",
    "    p1 (float): Belief to be in state 1\n",
    "    \n",
    "  Returns:\n",
    "    float: Expected reward.\n",
    "  \"\"\"\n",
    "  # TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bcbLWM5pWLWf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# With numpy, operations like +/-/*// are performed element-wise, \n",
    "# so unless you did something weird above, we can evaluate the values of all \n",
    "# beliefes in parallel.\n",
    "p1s = np.linspace(0,1)\n",
    "\n",
    "plt.plot(p1s, reward_b(p1s, action=1), color='red')\n",
    "plt.plot(p1s, reward_b(p1s, action=2), color='blue')\n",
    "plt.plot(p1s, reward_b(p1s, action=3), color='green')\n",
    "\n",
    "# We'll need this one later\n",
    "plt.plot(p1s, one_step_value(p1s), color='black', linestyle='--')\n",
    "\n",
    "plt.plot(p1s, value_taking_u3_first(p1s), color='black')\n",
    "\n",
    "plt.xlabel(\"Belief to be in state 1\")\n",
    "plt.ylabel(\"Expected 2-step Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gALU0G_JXtbi"
   },
   "source": [
    "**Question:** Again, does that make intuitive sense? Why? And is the resulting value function of 'best choices' piecewise linear and convex :)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HYVQrRh-Y0GO"
   },
   "source": [
    "# Chapter 2: From planning to Value Iteration #\n",
    "\n",
    "Now, those nice plots we have made so far are what we would like to get out of Value Iteration: Telling us exactly what the expected value for each action is: Each line corresponds to one action we can take now. This allows us to, for each possible belief, pick the action with the highest expected return. \n",
    "\n",
    "So far, we've created those plots by just picking a bunch of points (50 to be exact) and evaluate the value for all possible actions at all of those points. Of course, that scales terribly. For one, planning more and more into the future becomes costly. But more importantly, our simple POMDP has only two states. If it had more, the resulting belief distribution would become higher dimensional, requiring more and more points to cover it with sufficient accuracy. So what can we do?\n",
    "\n",
    "Well, we can realise that the resulting value function for any horizon length will always be convex (why?) and consist of a *finite* number of linear (why linear?) segments, in other words it is *piecewise linear and convex* (PWLC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fMDNpnG76bB"
   },
   "source": [
    "## 2.1 From Point to Lines ##\n",
    "\n",
    "Let's recap what we did above: How did we move from the 1-step case to 2 steps?\n",
    "\n",
    "1. For each belief $b$, we computed the new belief $b'$ *for each possible observation $z_i$ that we could receive* \n",
    "2. We evaluated the new belief $b'$ using our old `one_step_value` function\n",
    "2. We summed all those values up, weighted by how likely each observation was (as given by the `simulate` function). \n",
    "\n",
    "Now, we know that the value function is represented by a finite number of *lines*. Can we apply the same 3 steps to lines instead of points? Why? \n",
    "\n",
    "This is shown wonderfully in the [POMDP tutorial](http://pomdp.org/tutorial/pomdp-vi-example.html):\n",
    "\n",
    "We start with the Horizon 1 value function:\n",
    "\n",
    "![Horizon 1 value function](https://github.com/maximilianigl/pomdp-practical/blob/master/images/Tutorial_1.png?raw=true)\n",
    "\n",
    "Note that here the colors on the bottom represent which action is optimal.\n",
    "\n",
    "Next, for each action and for each possible observation, we transform this one step value function based on how likely this observation is and how this would affect our belief state. In this example, there are 3 possible observations, so we get three transformed value functions (for action $a_1$):\n",
    "\n",
    "![Transformed horizon 1  value function](https://github.com/maximilianigl/pomdp-practical/blob/master/images/Tutorial_2.png?raw=true)\n",
    "\n",
    "Now we need to sum those up. But how? Add all blue lines together? That would mean always taking $a_1$, independenlty of the observation. So instead, we can come up with different strategies like $(z_1:a_2, z_2:a_1, z_3:a_1)$, i.e. take $a_2$ when we see $z_1$, take $a_1$ for $z_2$ and so on. Here that would mean add the left green line, the middle blue line and the right blue line. But which strategy is best? Above, when we did it for a fixed initial belief, we just took the `max` operation given the new belief. However, now we need to think about *all* possible initial believes, so the optimal strategy depends on our *original* belief $b$ which is still shown on the $x$-axis. We can easily read off the best strategies like so: \n",
    "\n",
    "![Best response strategy](https://github.com/maximilianigl/pomdp-practical/blob/master/images/Tutorial_3.png?raw=true)\n",
    "\n",
    "where we see that there are 4 best strategies reacting to observations, depending on what is our starting belief. \n",
    "Each of those strategies tells us which lines to add up, given the resulting value function for taking action $a_1$:\n",
    "\n",
    "![Value function for a1](https://github.com/maximilianigl/pomdp-practical/blob/master/images/Tutorial_4.png?raw=true).\n",
    "\n",
    "Now we can do the same thing again with $a_2$ and take the max between the two and voila, we have the horizon 2 value function.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FTM47EHyDt_a"
   },
   "source": [
    "## 2.2 Playing around with Lines ##\n",
    "\n",
    "But grey is all theory, so let's just play around with Value Iteration. Because we're lazy and don't want to implement it from scratch, let's use [this](https://github.com/pemami4911/POMDPy) repo with 98 stars that implements Value Iteration for the Tiger problem, which is very similar to our setting: A tiger is behind one of two doors and you can either open one of the doors, or listen for a *roar*, giving you a noisy observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "vM0vShxjD4ga"
   },
   "outputs": [],
   "source": [
    "# Install a POMDP Value Iteration solver because \n",
    "# implementing that in half an afternoon is a bit ambitious\n",
    "\n",
    "!pip install git+https://github.com/pemami4911/POMDPy.git\n",
    "!pip install future\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PbeN9H27gmlK"
   },
   "source": [
    "### Running Value Iteration ###\n",
    "\n",
    "In the field below, set the planning horizon and run it. I'd recommend starting with one and working your way up ;). The next field allows you to plot the resulting value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "io9VUfBREVLA"
   },
   "outputs": [],
   "source": [
    "from pomdpy import Agent\n",
    "from pomdpy.solvers import ValueIteration\n",
    "from pomdpy.log import init_logger\n",
    "from examples.tiger import TigerModel\n",
    "\n",
    "planning_horizon = 1\n",
    "\n",
    "args = {\n",
    "    'planning_horizon': planning_horizon,\n",
    "    'n_epochs': 1,\n",
    "    'max_steps': 1,\n",
    "    'env': 'Tiger',\n",
    "    'solver': 'ValueIteration',\n",
    "    'discount': 1,\n",
    "    'save': True,\n",
    "}\n",
    "\n",
    "env = TigerModel(args)\n",
    "env.weight_dir = ('./')\n",
    "agent = Agent(env, ValueIteration)\n",
    "test = agent.discounted_return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rO9rKQY1ctUi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "ps = np.linspace(0,1)\n",
    "\n",
    "line = lambda p, v0, v1: p * v0 + (1-p) * v1\n",
    "\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "import pickle\n",
    "alpha_vectors = pickle.load(open('VI_planning_horizon_{}.pkl'.format(planning_horizon), 'rb'))\n",
    "\n",
    "\n",
    "print(\"There are {} alpha-vectors\".format(len(alpha_vectors)))\n",
    "for av in alpha_vectors:\n",
    "  print(vars(av))\n",
    "  plt.plot(ps, line(ps, av.v[0], av.v[1]), color = colors[av.action])\n",
    "  \n",
    "plt.xlabel(\"Belief to be in state 1\")\n",
    "plt.ylabel(\"Value\")\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zWVYW2VbhU4D"
   },
   "source": [
    "**Question:** Have you tried the above for several different `planning_horizon`s? Does the result look reasonable? Why not?\n",
    "\n",
    "Looks like there's a bug....\n",
    "\n",
    "**Bonus question:** Fix the bug. Below I copy-pasted his code for his value iteration solver for your convenience. Because the solver is passed into his `Agent` as argument, we can just replace it with our version. Tipp: Look for line 56, marked by `# TODO: Fix BUG`, that's where the magic happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "p2XXf5DqSd-T"
   },
   "outputs": [],
   "source": [
    "from pomdpy.solvers.solver import Solver\n",
    "from pomdpy.solvers.alpha_vector import AlphaVector\n",
    "from scipy.optimize import linprog\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "class ValueIteration(Solver):\n",
    "    def __init__(self, agent):\n",
    "        \"\"\"\n",
    "        Initialize the POMDP exact value iteration solver\n",
    "        :param agent:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(ValueIteration, self).__init__(agent)\n",
    "        self.gamma = set()\n",
    "        self.history = agent.histories.create_sequence()\n",
    "\n",
    "    @staticmethod\n",
    "    def reset(agent):\n",
    "        return ValueIteration(agent)\n",
    "\n",
    "    def value_iteration(self, t, o, r, horizon):\n",
    "        \"\"\"\n",
    "        Solve the POMDP by computing all alpha vectors\n",
    "        :param t: transition probability matrix\n",
    "        :param o: observation probability matrix\n",
    "        :param r: immediate rewards matrix\n",
    "        :param horizon: integer valued scalar represented the number of planning steps\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        discount = self.model.discount\n",
    "        actions = len(self.model.get_all_actions())  # |A| actions\n",
    "        states = self.model.num_states  # |S| states\n",
    "        observations = len(self.model.get_all_observations())  # |Z| observations\n",
    "        first = True\n",
    "\n",
    "        # initialize gamma with a 0 alpha-vector\n",
    "        dummy = AlphaVector(a=-1, v=np.zeros(states))\n",
    "        self.gamma.add(dummy)\n",
    "\n",
    "        # start with 1 step planning horizon, up to horizon-length planning horizon\n",
    "        for k in range(horizon):\n",
    "            # new set of alpha vectors to add to set gamma\n",
    "            gamma_k = set()\n",
    "            # Compute the new coefficients for the new alpha-vectors\n",
    "            v_new = np.zeros(shape=(len(self.gamma), actions, observations, states))\n",
    "            idx = 0\n",
    "            for v in self.gamma:\n",
    "                for u in range(actions):\n",
    "                    for z in range(observations):\n",
    "                        for j in range(states):\n",
    "                            for i in range(states):\n",
    "                                \n",
    "                                # TODO: Fix BUG\n",
    "                                # v_i_k * p(z | x_i, u) * p(x_i | u, x_j)                                 \n",
    "                                v_new[idx][u][z][i] += v.v[i] * o[u][i][z] * t[u][j][i]\n",
    "\n",
    "                idx += 1\n",
    "            # add (|A| * |V|^|Z|) alpha-vectors to gamma, |V| is |gamma_k|\n",
    "            for u in range(actions):\n",
    "                c = self.compute_indices(idx, observations)\n",
    "                for indices in c:  # n elements in c is |V|^|Z|\n",
    "                    for z in range(observations):\n",
    "                        temp = np.zeros(states)\n",
    "                        for i in range(states):\n",
    "                            temp[i] = discount * (r[u][i] + v_new[indices[z]][u][z][i])\n",
    "                        gamma_k.add(AlphaVector(a=u, v=temp))\n",
    "            self.gamma.update(gamma_k)\n",
    "            if first:\n",
    "                # remove the dummy alpha vector\n",
    "                self.gamma.remove(dummy)\n",
    "                first = False\n",
    "            self.prune(states)\n",
    "            #  plot_gamma(title='V(b) for horizon T = ' + str(k + 1), self.gamma)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_indices(k, m):\n",
    "        \"\"\"\n",
    "        Compute all orderings of m elements with values between [0, k-1]\n",
    "        :param k: Number of alpha-vectors\n",
    "        :param m: Number of observations\n",
    "        :return: list of lists, where each list contains m elements, and each element is in [0, k-1].\n",
    "        Total should be k^m elements\n",
    "        \"\"\"\n",
    "        x = list(range(k))\n",
    "        return [p for p in product(x, repeat=m)]\n",
    "\n",
    "    def prune(self, n_states):\n",
    "        \"\"\"\n",
    "        Remove dominated alpha-vectors using Lark's filtering algorithm\n",
    "        :param n_states\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # parameters for linear program\n",
    "        delta = 0.0000000001\n",
    "        # equality constraints on the belief states\n",
    "        A_eq = np.array([np.append(np.ones(n_states), [0.])])\n",
    "        b_eq = np.array([1.])\n",
    "\n",
    "        # dirty set\n",
    "        F = self.gamma.copy()\n",
    "        # clean set\n",
    "        Q = set()\n",
    "\n",
    "        for i in range(n_states):\n",
    "            max_i = -np.inf\n",
    "            best = None\n",
    "            for av in F:\n",
    "                if av.v[i] > max_i:\n",
    "                    max_i = av.v[i]\n",
    "                    best = av\n",
    "            Q.update({best})\n",
    "            F.remove(best)\n",
    "        while F:\n",
    "            av_i = F.pop()  # get a reference to av_i\n",
    "            F.add(av_i)  # don't want to remove it yet from F\n",
    "            dominated = False\n",
    "            for av_j in Q:\n",
    "                c = np.append(np.zeros(n_states), [1.])\n",
    "                A_ub = np.array([np.append(-(av_i.v - av_j.v), [-1.])])\n",
    "                b_ub = np.array([-delta])\n",
    "\n",
    "                res = linprog(c, A_eq=A_eq, b_eq=b_eq, A_ub=A_ub, b_ub=b_ub, bounds=(0, None))\n",
    "                if res.x[n_states] > 0.0:\n",
    "                    # this one is dominated\n",
    "                    dominated = True\n",
    "                    F.remove(av_i)\n",
    "                    break\n",
    "\n",
    "            if not dominated:\n",
    "                max_k = -np.inf\n",
    "                best = None\n",
    "                for av_k in F:\n",
    "                    b = res.x[0:2]\n",
    "                    v = np.dot(av_k.v, b)\n",
    "                    if v > max_k:\n",
    "                        max_k = v\n",
    "                        best = av_k\n",
    "                F.remove(best)\n",
    "                if not self.check_duplicate(Q, best):\n",
    "                    Q.update({best})\n",
    "        self.gamma = Q\n",
    "\n",
    "    @staticmethod\n",
    "    def check_duplicate(a, av):\n",
    "        \"\"\"\n",
    "        Check whether alpha vector av is already in set a\n",
    "\n",
    "        :param a:\n",
    "        :param av:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for av_i in a:\n",
    "            if np.allclose(av_i.v, av.v):\n",
    "                return True\n",
    "            if av_i.v[0] == av.v[0] and av_i.v[1] > av.v[1]:\n",
    "                return True\n",
    "            if av_i.v[1] == av.v[1] and av_i.v[0] > av.v[0]:\n",
    "                return True\n",
    "\n",
    "    @staticmethod\n",
    "    def select_action(belief, vector_set):\n",
    "        \"\"\"\n",
    "        Compute optimal action given a belief distribution\n",
    "        :param belief: dim(belief) == dim(AlphaVector)\n",
    "        :param vector_set\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        max_v = -np.inf\n",
    "        best = None\n",
    "        for av in vector_set:\n",
    "            v = np.dot(av.v, belief)\n",
    "\n",
    "            if v > max_v:\n",
    "                max_v = v\n",
    "                best = av\n",
    "\n",
    "        if best is None:\n",
    "            raise ValueError('Vector set should not be empty')\n",
    "\n",
    "        return best.action, best\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "POMDP_practical.ipynb",
   "provenance": [
    {
     "file_id": "1uGxAECYhFxDA8bPRTXZqg98ymcyzLaJI",
     "timestamp": 1550752592354
    },
    {
     "file_id": "https://github.com/maximilianigl/pomdp-practical/blob/master/POMDP_practical_solution.ipynb",
     "timestamp": 1550752545248
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
